{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA DRIVEN OPTIMIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo8FkWDxjx0K"
   },
   "source": [
    "**Exercise 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 42,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "id": "ygUslOgTioRC",
    "outputId": "4ef6eb03-90c3-4f89-ed01-f220043a4eca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-a32fd286-f1f6-4655-bbe4-0e2c9fed1686\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-a32fd286-f1f6-4655-bbe4-0e2c9fed1686\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "%matplotlib inline \n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6X4xrLyKj7kC"
   },
   "source": [
    "1(i) Read the following image (zebra.jpg) into a Python notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Q3gzDQHioNG"
   },
   "outputs": [],
   "source": [
    "Zeb = cv2.imread('Zebra.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJBJbXSPn9M7"
   },
   "source": [
    "1(ii) Convert the colored image into a gray level image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZHNCDXHioDr"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,10))\n",
    "ax.grid(False)\n",
    "gray = np.mean(Zeb, -1)\n",
    "plt.imshow(gray,'gray')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-knOreSzoj5i"
   },
   "source": [
    "1(iii) Show how many unique elements does image matrix from (ii) have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyCv8rukin39"
   },
   "outputs": [],
   "source": [
    "print(np.unique(gray))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0zvWBf8pEcN"
   },
   "source": [
    "1(iv) Perform the SVD on the image-matrix of the gray-level image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hk4QaNyOinsl"
   },
   "outputs": [],
   "source": [
    "U,S,VT = np.linalg.svd(gray, full_matrices = False)\n",
    "S = np.diag(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZSn4BcMphWC"
   },
   "source": [
    "1(v) Approximate gray level image matrix by its SVD from (iv) taking different numbers of singular values to display the (a) 30% (b) 50% (c) 80% compressions of the image,respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LznJJw1spDOc"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for k in (30,50,80):\n",
    "    Zebapprox = U[:,:k] @ S[0:k,:k] @ VT[:k,:]\n",
    "    plt.figure(i+1)\n",
    "    i += 1\n",
    "    image = plt.imshow(256-Zebapprox)\n",
    "    image.set_cmap('gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('k = ' + str(k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajkek4QfinR7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7dVCq6gixOB"
   },
   "source": [
    "**Exercise 2** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SuC1xTQq6WZ"
   },
   "source": [
    "Given the Rosenbrock function $f : \\mathbb{R}^n → \\mathbb{R}$\n",
    "$$ f (x) =\n",
    "\\sum_ {i=1}^{n−1} [\n",
    "100 (x {i+1} − x_i^2)^2\n",
    "+ (1 − x_i )^2]\n",
    "i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jQ9hLA6J4z0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfBkAF7Gi4BL"
   },
   "source": [
    "2(i) Use jupyter to define $f (x)$ under the PyTorch machine learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3O_o39q-bTP2"
   },
   "outputs": [],
   "source": [
    "def Rosenbrock (x):\n",
    "  Sum = 0\n",
    "  n = 10\n",
    "  for i in range(n-1):\n",
    "    Sum = Sum + 100* (x[i+1]-x[i]**2)**2 + (1-x[i])**2\n",
    "  return Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQBLMMZ5jAt2"
   },
   "source": [
    "2(ii) Write a Python function to computer the gradient of this function for $n = 10$ using the\n",
    "Automatic differentiation toolbox under PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9pNDJycctHf"
   },
   "outputs": [],
   "source": [
    "def Grad(func,x):\n",
    "  return torch.autograd.grad(func,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNvl7LQ7jHdY"
   },
   "source": [
    "2(iii) Evaluate the gradient at the point $x = (1, 1, \\cdots , 1) ∈ R^{10}$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nk7fecW2dWFb"
   },
   "outputs": [],
   "source": [
    "x = torch.ones(10,requires_grad=True,dtype = torch.float32)\n",
    "Grad(Rosenbrock(x),x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw37UXust8rS"
   },
   "source": [
    "Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFD_oVijuWNL"
   },
   "source": [
    "**Web searching: What is the Google Matrix? What is Page Ranking (in the perspective of Matrix Algebra)?**\n",
    "**Google Matrix:** \n",
    "\n",
    "Google matrix is a stochastic matrix that Google's PageRank algorithm employs. The matrix depicts a graph, with the edges indicating page connections. Using the power technique, the PageRank of each page may then be created repeatedly from the Google matrix. The matrix must, however, be irreducible, aperiodic, and stochastic in order for the power technique to converge.\n",
    "\n",
    "**How Google Build its Matrix:** \n",
    "\n",
    "Google divides the web into pieces, crawls these segments, and adds them to their main index, which is stored on hundreds of servers. This technique is repeated every day to maintain Google's web index current. Now, when a user goes to Google and puts in a question, the Google search engine sets to work finding the most relevant and crucial web sites to display in relation to the query. The query is first broken down into the individual terms entered into the search engine. Google then uses spiders to browse through Google's index in search of sites that contain the phrases, which they do across numerous machines. These spiders appear on many pages to begin with. They continue their search by following the links on the current page to other pages, and so on, until all pages relevant to the query have been indexed. All of these pages are combined, and Google uses over a hundred different ranking factors to sort the resulting pages based on overall rank, including page quality (authoritative, low quality, or spam), word location (title, url, etc.), word proximity (whether words are next to each other in a sentence or not), time users have spent on the pages before, and so on.\n",
    "\n",
    "$G = [G_{ij}]$ is the fundamental Google matrix, which is defined as follows:\n",
    "If $deg(i)$ is greater than zero, $G_{ij} = 1/deg(i)$ for each page $j = 1$ that may be reached directly from page $i$ all other $G$ entries in row $i$ are zeros.\n",
    "If deg(i) equals 0 (a dangling node), then $G_{ij} = 1/n$ $\\forall j = 1, \\cdots , n$\n",
    "\n",
    "**PAGERANK ALGORITHM**\n",
    "\n",
    "PageRank is a nonnegative $n$-vector in which each entry is understood as a measure of the relevance of the webpages that correspond to it. One technique to determine the relevance of a page is to consider the chance that a random user will arrive at that page after an unlimited number of clicks. PageRank is a nonnegative left eigenvector $y$ of $G$ associated with the eigenvalue $1$, that is, $y^TG = y^T$ (normalized so that $y^T e = 1$).\n",
    "\n",
    "The PageRank algorithm, which was devised by Google's founders, is the most important factor in determining a page's overall ranking. The PageRank algorithm is the major criteria utilized to evaluate the most reputable and authoritative pages across the index during the search process. The development of the PageRank algorithm was what distinguished Google from the competition early on and helped it become the most successful and powerful search engine to date. The PageRank algorithm transformed the way search engines retrieved web pages and showed them in true order of importance.\n",
    "\n",
    "It's helpful to see these networks of hyperlinks connecting web sites as directed graphs. It turns out that the tools needed to determine web page rankings using the PageRank method are linear algebra and graph theory. The purpose of this paper is to explain the mathematics that underpin Google's PageRank algorithm. We go into the basics of Google's PageRank algorithm, including a review of key linear algebra and graph theory principles relevant to this process. By the conclusion of the article, the reader should have a basic grasp of how Google's PageRank algorithm calculates web page rankings and how to interpret the results.\n",
    "\n",
    "**PageRank** is calculated by computing a normalized nonnegative solution of the system of linear equations $y^T G = y^T$. The good news is that a solution is always available; the bad news is that several options may exist. Even if there is a unique solution, typical approaches like the power method may fail to find it because $G$ contains one or more eigen-values other than $1$ with modulus $1$. The conventional way to solve these problems is to change $G:$ for a given $c \\in [0, 1]$ and a given nonnegative $n$-vector $v$ such that $v^T e = 1$, defining the parametric Google matrix equation.\n",
    "$$\n",
    "G(c) = cG + (1-c)ev^T\n",
    "$$\n",
    "which describes the following user behavior: A person visiting page $j$ follows the rule defined by the fundamental Google matrix $G$ with probability $c$; the user follows the rule described by the nonnegative probability vector $1-c$. (i.e., with probability $v_j$, advances to page $j$)\n",
    "\n",
    "**MATHS BEHIND GOOGLE’S PAGERANK ALGORITHM**\n",
    "\n",
    "(a). **Graph Theory**: Graph theory is one of the most important foundations for modeling the Internet network. The study of graphs, mathematical structures that model pairwise relationships between objects, is known as graph theory. Vertices and edges make up graphs, with vertices being points on the graph and edges being the lines that connect them. In an undirected graph, the direction of an edge between two vertices is not distinguished; in a directed graph, edges have directions. \n",
    " \n",
    " From modeling atomic structures to modeling traffic networks, graphs have been utilized in a variety of domains and for a variety of objectives. They've also proved important in computer science, and they're a good way to change the structure of the Internet. The way the Internet is modeled is where graph theory comes into play. The Internet is made up of many webpages, each with its own set of information. Links to additional pages may be found on each page. This can be represented using graph theory, in which each page is a vertex, and the edges connecting each vertex are equivalent to the linkages between pages.\n",
    "\n",
    "(b). **Markov Chains:**  A non-negative $(n times n)$ matrix with each row summing to $1$ is called a row-stochastic matrix. It's significant because it's used in Markov chains. A Markov chain is a stochastic process that meets the Markov property, which states that the likelihood of future behavior is independent of past behavior. The transition probability can also be used to define the Internet network matrix:\n",
    "\\begin{align*}\n",
    "S_{ij} = P(X_t = p_j | X_{t-1} = p_i )\n",
    "\\end{align*}\n",
    "where each entry can be described as the odds that a web surfer would followa link to page $p_j$, given that they were on page $p_i$, Since: $S= [S_{ij}]$, then $S$ is a transition matrix.\n",
    "\n",
    "The limiting behavior of the random variable sequence is one of the most important concepts in the Markov chain model. The limiting distribution is a row vector with the same size as the state space and an item $i$ that represents the amount of time the system spends in state $i$ over time.\n",
    "\n",
    "(c). **Stochastic Matrices**: There are several properties that can be known from a stochastic matrix.Thespectral radius of a matrix is defined as:\n",
    "\\begin{align*}\n",
    "\\rho (A) = max_{\\lambda \\in \\sigma (A)}|A|,\n",
    "\\end{align*}\n",
    "Where$\\rho(A)$is the set of distinct eigenvalues of the matrix. Theinfinite normofa matrix is defined as:\n",
    "\\begin{align*}\n",
    "||A||_{\\infty} = max_i \\sum_{j} |a_{ij}|\n",
    "\\end{align*}\n",
    "The norm creates an upper bound on$\\rho(A)$, so it is also true that:\n",
    "\n",
    "\\begin{align*}\n",
    "\\rho(A) \\leq ||A||\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUcWkkWtvn9-"
   },
   "source": [
    "**Exercise 4**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LF8YQPLxxxw4"
   },
   "source": [
    "\n",
    "**How does Netflix use the Singular Value Decomposition? Elaborate? What are \"recommender\n",
    "systems\" and \"collaborative filtering\" (in the perspective of Matrix Algebra)?**\n",
    "\n",
    "Netflix launched a competition on October 2nd, 2006 to find a more accurate movie recommendation system to replace their current algorithm, Cinematch. They announced a one-million-dollar prize to anyone who could enhance the Cinematch system by at least $10%$ Root Mean Squared inaccuracy (RSME). The grand prize was granted to team \"Bellkor's Pragmatic Chaos\" in 2009, after three years of competition.\n",
    "\n",
    "Netflix's motivation is to help consumers locate movies and series that suit their likes so that they will be more inclined to keep their subscription.\n",
    "\n",
    " In the subject of Collaborative Filtering, Singular Value Decompositions (SVD) have grown quite popular. A number of $SVD$ and $ SVD++$ models, were combined with Restricted Boltzmann Machines in the winning entry for the prestigious Netflix Prize. They were able to improve the accuracy of Netflix's existing algorithm by $10%$ using these strategies.\n",
    "\n",
    "**Recommender Systems**\n",
    "\n",
    "Depending on their strategy, recommender systems can be divided into two categories. The content filtering strategy is one of them, and it works by creating a profile for each user or product in order to categorise them. This is dependent on the ability to gather external data.\n",
    "\n",
    "The col-laborative filtering strategy, on the other hand, focuses on assessing relationships between users and products, as well as interdependencies between these groups. These connections are used to create new ones. Each strategy has its own set of advantages and disadvantages. Collaborative filtering, for example, is more accurate than content filtering, although it has a \"cold-start problem.\" This is because collaborative filtering requires additional information about the user in relation to the products, which is currently unavailable for new accounts.\n",
    "Neighborhood methods and latent factor models are two forms of recommender systems that can be further differentiated. In order to build a taste, the neighborhood technique \"neighbors\" objects rated by the same user. The system then compares these previously rated goods to ratings from other users on the same items to identify users with similar tastes. The system then considers relevant things that these other users scored well and recommends them to the original user based on these similarities. On the other hand, latent factor models seek to assess the user-item interaction by using components that are either inferred or explicitly gathered.\n",
    "\n",
    "\n",
    "**Data**\n",
    "\n",
    "These models rely on a reliant of data collected from service users. Explicit feedback is one of the most visible examples of this information. This is the information that the service receives from its users. Users rate the movies and television programmes they watch on Netflix. From one to five, this is assigned a numerical value. Statistics provided by the user about themselves, such as gender, age, the episodes and movies they choose to put in their queue, and so on, are also examples of this data kind.\n",
    "\n",
    "**Matrix Factorization**\n",
    "\n",
    "Matrix factorizations (sometimes called matrix decompositions) are useful for mapping users and things in factor spaces. User-item interactions are described as inner products in a dimensional latent factor space.\n",
    "A vector $q_i \\in R^f$ is assigned to each item. Each user is connected with a vector $p_u \\in R^f$, and the elements of the vector $p_u$ include characterizations of the user's level of interest in highly correlated items, whether this interest is positive or negative. In the estimate, the inner product of the two vectors approximates the users' rating of any single item $i$, denoted as $r_{ui}$. \n",
    "\\begin{align*}\n",
    "r_{ui} \\simeq q_{i}^* p_u\n",
    "\\end{align*}\n",
    "It's a pretty straightforward process for the system to apply the above equation to predict the rating a user would give to a given item after mapping the vectors that characterize the user item interaction.\n",
    "\n",
    "**Adding Biases**\n",
    "\n",
    "It is common for two goods with the same rating to be of different quality. One example would be a film that has a higher quality rating from reviewers, a higher expense and effort put into making the picture, and so on. As a result, ratings alone are insufficient to identify which films should be suggested, and this is where biases come into play. This is simply a characterization of an item's base quality, which is independent of the rating analysis performed and serves to better market movies that are regarded to be of greater quality than others. This  first-order approximation of this bias is:\n",
    "\\begin{align*}\n",
    "b_{ui} = \\mu + b_i + b_u\n",
    "\\end{align*}\n",
    "Other inputs can also affect the outcome of the recommendation such as Implict data,attributes such as income, gender, address, etc. Taking into consideration these inputs and forming the set of attributes $A(u)$ which is a set of Boolean attributes of user $u$ which gives us a new approximation for the expected rating of user $u$ of item $i$ as\n",
    "\\begin{align*}\n",
    "r_{ui}= \\mu + b_i + b_u + q_{i}^{*}  \\left[ p_u + |N(u)|^{-0.5} \\sum_{i \\in N(u)} x_i + \\sum_{a\\in A(u)} (y_a) \\right] \n",
    "\\end{align*}\n",
    "**Time-Based Factors**\n",
    "\n",
    "The model has only dealt with inputs that are now available and has generated suggestions based on a time-independent model. However, the model's accuracy can be improved by taking into account changes in user preference over time. The following terms are time dependent among those in our model: user preferences, user biases, and item biases.\n",
    "$$\n",
    "r_{ui}= \\mu + b_i (t) + b_u (t) + q_{i}^* p_u (t)\n",
    "$$\n",
    "**Learning Algorithm**\n",
    "\n",
    "Over time, a smart model can update its estimates of the inter dependencies between the user and the object, as well as \"learn.\" In light of this, the regularized squared error on a set of known ratings is\n",
    "$$\n",
    "min_{q^{*}p^{*} \\sum_{(u,i)\\in k} (r_{ui}-q_{i}^* p_u)^2 + \\lambda(||q_{i}||^2 + ||p_u||^2)\n",
    "$$\n",
    "This function minimizes the sum which is the difference between the approximated ratingsand  the  actual  ratings  squared.\n",
    "\n",
    "\n",
    "**Solution of SVD**\n",
    "\n",
    "We've looked at the components of a solid recommender system and even the winning model, $SVD++$, so far. However, it is unclear how these models relate to $SVD$, especially since rating calculations involve vectors of user and item interactions. These models are based on $SVD$ because it is utilized to create the vectors $p_u$ and $q_i$. When the Netflixprize competition first started, they disclosed the ratings of over $400,000$ anonymous members, totaling over $100 million$. This massive matrix is basically a collection of users as rows or columns, with items on the opposite side. Ratings are kept at the intersection of the user and the item, with each value in the matrix ranging from $1$ to $5$ for rated content and $0$ for unrated films. After denoting the rating matrix as $A$, the next step is to form $A$ and perform a singular value decomposition on it.\n",
    "\n",
    "When the $SVD$ is applied to $A$, a lot of information about user-item interactions is collected from the matrix. The user taste vector pu, which is a vector that characterizes the level of interest the user has in things that are high on the related criteria, is created as a result of this process. The other vector created is $q_i$, which determines whether or not an item have specific characteristics. These are created by combining $SVD$ with Principal Component Analysis $(PCA)$.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "It is evident that $SVD's$ applicability in recommender systems are quite powerful and diverse. It makes intuitive sense that an SVD-based model would be a good fit for a recommender system, because one of $SVD's$ main strengths is its ability to determine the relatedness of objects based on a set of variables. $SVD$ is so potent that it appears in practically every top entry for the Netflix prize \\."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f-vm4SsMBib"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "akande_adetoye_DDOMLA1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
